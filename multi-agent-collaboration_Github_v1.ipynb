{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fd1948-b5c3-48c4-b10e-2ae7e8c83334",
   "metadata": {},
   "source": [
    "# Basic Multi-agent Collaboration\n",
    "\n",
    "A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like `gpt-4`, it can be less effective at using many tools. \n",
    "\n",
    "One way to approach complicated tasks is through a \"divide-and-conquer\" approach: create an specialized agent for each task or domain and route tasks to the correct \"expert\".\n",
    "\n",
    "This notebook (inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155), by Wu, et. al.) shows one way to do this using LangGraph.\n",
    "\n",
    "The resulting graph will look something like the following diagram:\n",
    "\n",
    "![multi_agent diagram](./img/simple_multi_agent_diagram.png)\n",
    "\n",
    "Before we get started, a quick note: this and other multi-agent notebooks are designed to show _how_ you can implement certain design patterns in LangGraph. If the pattern suits your needs, we recommend combining it with some of the other fundamental patterns described elsewhere in the docs for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d7b6dcc-c985-46e2-8457-7e6b0298b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langchain_openai langsmith pandas langchain_experimental matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "743c19df-6da9-4d1e-b2d2-ea40080b9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/cookbook\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"your-key\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"your-key\"\n",
    "os.environ[\"TAVILY_API_KEY\"]=\"your-key\"\n",
    "\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-agent Collaboration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c91c3-c249-471d-b259-41975faa83fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e4344a7-21df-4d54-90d2-9d19b3416ffb",
   "metadata": {},
   "source": [
    "## Create Agents\n",
    "\n",
    "The following helper functions will help create agents. These agents will then be nodes in the graph.\n",
    "\n",
    "You can skip ahead if you just want to see what the graph looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4325a10e-38dc-4a98-9004-e1525eaba377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" Double check the answer if all the code is complete and runnable. You have to be completely sure nothing is missing\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: Use {tool_names} to gather data.\\n Use {system_message} to guide you in your task.\"\n",
    "            ),\n",
    "            (\"system\",\"{messages}\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b40de2-5dd4-4d5b-882e-577210723ff4",
   "metadata": {},
   "source": [
    "## Define tools\n",
    "\n",
    "We will also define some tools that our agents will use in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca076f3b-a729-4ca9-8f91-05c2ba58d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Annotated\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import Tool\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"]\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `plt.show(...)`. You must show the output to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Succesfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b54c0c-0b09-408b-abc5-86308929afb6",
   "metadata": {},
   "source": [
    "## Create graph\n",
    "\n",
    "Now that we've defined our tools and made some helper functions, will create the individual agents below and tell them how to talk to each other using LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a8c3c-86a0-46aa-b970-ab070fb787d9",
   "metadata": {},
   "source": [
    "### Define State\n",
    "\n",
    "We first define the state of the graph. This will just a list of messages, along with a key to track the most recent sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "290c91d4-f6f4-443c-8181-233d39102974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
    "\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# This defines the object that is passed between each node\n",
    "# in the graph. We will create different nodes for each agent and tool\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a283e-ea04-40c1-b792-f9e5f7d81203",
   "metadata": {},
   "source": [
    "### Define Agent Nodes\n",
    "\n",
    "We now need to define the nodes. First, let's define the nodes for the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71b790ca-9cef-4b22-b469-4b1d5d8424d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "# Helper function to create a node for a given agent\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    # We convert the agent output into a format that is suitable to append to the global state\n",
    "    if isinstance(result, FunctionMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # Since we have a strict workflow, we can\n",
    "        # track the sender so we know who to pass to next.\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "llm = ChatOpenAI(base_url=\"https://api-inference.huggingface.co/v1\",api_key=\"your-key\",\\\n",
    "                 model=\"google/gemma-2b-it\",temperature=0.05)\n",
    "\n",
    "# Research agent and node\n",
    "research_agent = create_agent(\n",
    "    llm,\n",
    "    [tavily_tool],\n",
    "    system_message=\"You should provide accurate data for the chart generator to use.\",\n",
    ")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"assistant\")\n",
    "\n",
    "# Chart Generator\n",
    "chart_agent = create_agent(\n",
    "    llm,\n",
    "    [python_repl],\n",
    "    system_message=\"This is a safe environment, please run the code and show the chart to the user.\",\n",
    ")\n",
    "chart_node = functools.partial(agent_node, agent=chart_agent, name=\"Chart Generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7f1b2-24a3-4340-bcb2-feb22e344fb6",
   "metadata": {},
   "source": [
    "### Define Tool Node\n",
    "\n",
    "We now define a node to run the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9a79c76-5c7c-42f6-91cf-635bc8305804",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tavily_tool, python_repl]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "\n",
    "def tool_node(state):\n",
    "    \"\"\"This runs tools in the graph\n",
    "\n",
    "    It takes in an agent action and calls that tool and returns the result.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    tool_input = json.loads(\n",
    "        last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "    )\n",
    "    # We can pass single-arg inputs by value\n",
    "    if len(tool_input) == 1 and \"__arg1\" in tool_input:\n",
    "        tool_input = next(iter(tool_input.values()))\n",
    "    tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_input,\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(\n",
    "        content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb30498-dbc4-4b20-980f-da08ebc9da56",
   "metadata": {},
   "source": [
    "### Define Edge Logic\n",
    "\n",
    "We can define some of the edge logic that is needed to decide what to do based on results of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f4b4d37-e8a3-4abb-8d42-eaea26016f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either agent can decide to end\n",
    "def router(state):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        # The previus agent is invoking a tool\n",
    "        return \"call_tool\"\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return \"end\"\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9359c34-e191-43a2-a3d4-f2dea636dfd2",
   "metadata": {},
   "source": [
    "### Define the Graph\n",
    "\n",
    "We can now put it all together and define the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4dce3901-6ad5-4df5-8528-6e865cf96cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"assistant\", research_node)\n",
    "workflow.add_node(\"Chart Generator\", chart_node)\n",
    "workflow.add_node(\"call_tool\", tool_node)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    router,\n",
    "    {\"continue\": \"Chart Generator\", \"call_tool\": \"call_tool\", \"end\": END},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"Chart Generator\",\n",
    "    router,\n",
    "    {\"continue\": \"assistant\", \"call_tool\": \"call_tool\", \"end\": END},\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    # Each agent node updates the 'sender' field\n",
    "    # the tool calling node does not, meaning\n",
    "    # this edge will route back to the original agent\n",
    "    # who invoked the tool\n",
    "    lambda x: x[\"sender\"],\n",
    "    {\n",
    "        \"assistant\": \"assistant\",\n",
    "        \"Chart Generator\": \"Chart Generator\",\n",
    "    },\n",
    ")\n",
    "workflow.set_entry_point(\"assistant\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9447e7-9ab6-43eb-8ae6-9b52f8ba8425",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "With the graph created, you can invoke it! Let's have it chart some stats for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6dc4510c-f37d-41c1-8b5a-47dd7bb6bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assistant': {'messages': [HumanMessage(content='**Using tavily_search_results_json:**\\n\\n```python\\nimport json\\n\\n# Get the data from the JSON file\\ndata = json.load(open(\"gdp_uk_5years.json\"))\\n\\n# Create a chart object\\nchart = tavily_search_results_json.Chart(data)\\n\\n# Generate the chart\\nchart.generate_chart()\\n\\n# Save the chart as a PNG file\\nchart.save_chart(\"gdp_uk_', name='assistant')], 'sender': 'assistant'}}\n",
      "----\n",
      "{'Chart Generator': {'messages': [HumanMessage(content='**FINAL ANSWER**\\n\\n```python\\nimport json\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\ndata = json.load(open(\"gdp_uk_5years.json\"))\\n\\n# Create a chart object\\nchart = tavily_search_results_json.Chart(data)\\n\\n# Generate the chart\\nchart.generate_chart()\\n\\n# Save the chart as a PNG file\\nchart.save_chart(\"gdp', name='Chart Generator')], 'sender': 'Chart Generator'}}\n",
      "----\n",
      "{'__end__': {'messages': [HumanMessage(content=\"Fetch the UK's GDP over the past 5 years, then draw a line graph of it. Once you code it up, finish.\"), HumanMessage(content='**Using tavily_search_results_json:**\\n\\n```python\\nimport json\\n\\n# Get the data from the JSON file\\ndata = json.load(open(\"gdp_uk_5years.json\"))\\n\\n# Create a chart object\\nchart = tavily_search_results_json.Chart(data)\\n\\n# Generate the chart\\nchart.generate_chart()\\n\\n# Save the chart as a PNG file\\nchart.save_chart(\"gdp_uk_', name='assistant'), HumanMessage(content='**FINAL ANSWER**\\n\\n```python\\nimport json\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\ndata = json.load(open(\"gdp_uk_5years.json\"))\\n\\n# Create a chart object\\nchart = tavily_search_results_json.Chart(data)\\n\\n# Generate the chart\\nchart.generate_chart()\\n\\n# Save the chart as a PNG file\\nchart.save_chart(\"gdp', name='Chart Generator')], 'sender': 'Chart Generator'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Fetch the UK's GDP over the past 5 years,\"\n",
    "                \" then draw a line graph of it.\"\n",
    "#                \"Run the code and show the line graph to the user.\"\n",
    "                \" Once you code it up, finish.\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 150},\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c6ee0f0-ea57-4b88-8133-a1e835f5bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\\nimport json\\n\\n# Get the data from the JSON file\\ndata = json.load(open(\"gdp_uk_5years.json\"))\\n\\n# Create a chart object\\nchart = tavily_search_results_json.Chart(data)\\n\\n# Generate the chart\\nchart.generate_chart()\\n\\n# Save the chart as a PNG file\\nchart.save_chart(\"gdp_uk_', name='assistant'), HumanMessage(content='**FINAL ANSWER**\\n\\n```python\\nimport json\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\ndata = json.load(open(\"gdp_uk_5years.json\"))\\n\\n# Create a chart object\\nchart = tavily_search_results_json.Chart(data)\\n\\n# Generate the chart\\nchart.generate_chart()\\n\\n# Save the chart as a PNG file\\nchart.save_chart(\"gdp', name='Chart Generator')\n"
     ]
    }
   ],
   "source": [
    "text=str(s[\"__end__\"][\"messages\"])\n",
    "code_start = text.find(\"```python\")\n",
    "code_end = text.find('sender')\n",
    "code = text[code_start + 8:code_end].strip()\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "176a99b0-b457-45cf-8901-90facaa852da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assistant': {'messages': [HumanMessage(content=\"```python\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\ndata = tavily_search_results_json.get_data()\\n\\n# Filter the data to only include variables named 'x' and 'y'\\nvariables = [variable for variable in data if variable['name'] == 'x' or variable['name'] == 'y']\\n\\n# Calculate the sum of the 'x' and 'y' variables\\nsum = float\", name='assistant')], 'sender': 'assistant'}}\n",
      "----\n",
      "{'Chart Generator': {'messages': [HumanMessage(content=\"```python\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\ndata = tavily_search_results_json.get_data()\\n\\n# Filter the data to only include variables named 'x' and 'y'\\nvariables = [variable for variable in data if variable['name'] == 'x' or variable['name'] == 'y']\\n\\n# Calculate the sum of the 'x' and 'y' variables\\nsum = float\", name='Chart Generator')], 'sender': 'Chart Generator'}}\n",
      "----\n",
      "{'assistant': {'messages': [HumanMessage(content=\"FINAL ANSWER\\n\\n```python\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\nndata = tavily_search_results_json.get_data()\\n\\n# Filter the data to only include variables named 'x' and 'y'\\nvariables = [variable for variable in data if variable['name'] == 'x' or variable['name'] == 'y']\\n\\n# Calculate the sum of the 'x' and 'y' variables\\n\", name='assistant')], 'sender': 'assistant'}}\n",
      "----\n",
      "{'__end__': {'messages': [HumanMessage(content='Give me the sum of two variables, x and y, x = 333 and y = 444. Once you code it up, finish.'), HumanMessage(content=\"```python\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\ndata = tavily_search_results_json.get_data()\\n\\n# Filter the data to only include variables named 'x' and 'y'\\nvariables = [variable for variable in data if variable['name'] == 'x' or variable['name'] == 'y']\\n\\n# Calculate the sum of the 'x' and 'y' variables\\nsum = float\", name='assistant'), HumanMessage(content=\"```python\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\ndata = tavily_search_results_json.get_data()\\n\\n# Filter the data to only include variables named 'x' and 'y'\\nvariables = [variable for variable in data if variable['name'] == 'x' or variable['name'] == 'y']\\n\\n# Calculate the sum of the 'x' and 'y' variables\\nsum = float\", name='Chart Generator'), HumanMessage(content=\"FINAL ANSWER\\n\\n```python\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\nndata = tavily_search_results_json.get_data()\\n\\n# Filter the data to only include variables named 'x' and 'y'\\nvariables = [variable for variable in data if variable['name'] == 'x' or variable['name'] == 'y']\\n\\n# Calculate the sum of the 'x' and 'y' variables\\n\", name='assistant')], 'sender': 'assistant'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#break\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Give me the sum of two variables, x and y,\"\n",
    "                \" x = 333 and y = 444.\"\n",
    "                \" Once you code it up, finish.\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 150},\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "010fc36e-4116-4758-bcac-b02c7dcd405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\\nimport tavily_search_results_json\\n\\n# Get the data from the JSON file\\nndata = tavily_search_results_json.get_data()\\n\\n# Filter the data to only include variables named 'x' and 'y'\\nvariables = [variable for variable in data if variable['name'] == 'x' or variable['name'] == 'y']\\n\\n# Calculate the sum of the 'x' and 'y' variables\\n\"\n"
     ]
    }
   ],
   "source": [
    "text=str(s[\"__end__\"][\"messages\"][-1])\n",
    "code_start = text.find(\"```python\")\n",
    "code_end = text.find(\"```\", code_start + 8)\n",
    "code = text[code_start + 8:code_end-15].strip()\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f9b469b-f498-4319-a5dc-bce872ea4591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Succesfully executed:\\n```python\\nn\\\\nimport tavily_search_results_json\\\\n\\\\n# Get the data from the JSON file\\\\nndata = tavily_search_results_json.get_data()\\\\n\\\\n# Filter the data to only include variables named \\'x\\' and \\'y\\'\\\\nvariables = [variable for variable in data if variable[\\'name\\'] == \\'x\\' or variable[\\'name\\'] == \\'y\\']\\\\n\\\\n# Calculate the sum of the \\'x\\' and \\'y\\' variables\\\\n\"\\n```\\nStdout: SyntaxError(\\'unexpected character after line continuation character\\', (\\'<string>\\', 1, 3, \\'n\\\\\\\\nimport tavily_search_results_json\\\\\\\\n\\\\\\\\n# Get the data from the JSON file\\\\\\\\nndata = tavily_search_results_json.get_data()\\\\\\\\n\\\\\\\\n# Filter the data to only include variables named \\\\\\'x\\\\\\' and \\\\\\'y\\\\\\'\\\\\\\\nvariables = [variable for variable in data if variable[\\\\\\'name\\\\\\'] == \\\\\\'x\\\\\\' or variable[\\\\\\'name\\\\\\'] == \\\\\\'y\\\\\\']\\\\\\\\n\\\\\\\\n# Calculate the sum of the \\\\\\'x\\\\\\' and \\\\\\'y\\\\\\' variables\\\\\\\\n\"\\\\n\\'))'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_repl(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179640a-a0bf-4249-91a1-ed6b2902b009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
